{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use talon to extract the email signatures as training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Vince,\n",
      "\n",
      "Thank you for registering for Henwood's ERCOT Symposium on January 23, 2001.\n",
      "We are pleased to confirm your attendance.  Attached is a copy of the\n",
      "program agenda.  As indicated, registration will begin at 9:30am.  The\n",
      "program begins at 10:00am and runs until 3:00pm.  Lunch along with\n",
      "refreshments will be provided. Demonstrations of Henwood's software\n",
      "applications and eBusiness solutions will be available following the\n",
      "workshop for interested parties.\n",
      "\n",
      "Directions to the Hyatt Regency Houston are attached for your convenience.\n",
      "Please do not hesitate to contact me with any questions or concerns that you\n",
      "may have.\n",
      "\n",
      "We look forward to seeing you in Houston!\n",
      "\n",
      "Heather Mason\n",
      "Marketing Assistant\n",
      "Henwood Energy Services, Inc.\n",
      "2710 Gateway Oaks Dr.\n",
      "Suite 300 N\n",
      "Sacramento, CA 95833\n",
      "Phone: (916) 569-0985\n",
      "Fax: (916) 569-0999\n"
     ]
    }
   ],
   "source": [
    "import talon\n",
    "from talon import quotations\n",
    "talon.init()\n",
    "text = \"\"\"\n",
    "Dear Vince,\n",
    "\n",
    "Thank you for registering for Henwood's ERCOT Symposium on January 23, 2001.\n",
    "We are pleased to confirm your attendance.  Attached is a copy of the\n",
    "program agenda.  As indicated, registration will begin at 9:30am.  The\n",
    "program begins at 10:00am and runs until 3:00pm.  Lunch along with\n",
    "refreshments will be provided. Demonstrations of Henwood's software\n",
    "applications and eBusiness solutions will be available following the\n",
    "workshop for interested parties.\n",
    "\n",
    "Directions to the Hyatt Regency Houston are attached for your convenience.\n",
    "Please do not hesitate to contact me with any questions or concerns that you\n",
    "may have.\n",
    "\n",
    "We look forward to seeing you in Houston!\n",
    "\n",
    "Heather Mason\n",
    "Marketing Assistant\n",
    "Henwood Energy Services, Inc.\n",
    "2710 Gateway Oaks Dr.\n",
    "Suite 300 N\n",
    "Sacramento, CA 95833\n",
    "Phone: (916) 569-0985\n",
    "Fax: (916) 569-0999\n",
    "\"\"\"\n",
    "\n",
    "reply = quotations.extract_from_plain(text)\n",
    "from talon import signature\n",
    "from talon.signature.bruteforce import extract_signature\n",
    "text, signature = extract_signature(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talon results are not always good and can only apply certain well-formated cases. So pick the email signature corpus manually. ~~First, use the Python email package to extract the main body (including the signature). Then apply the talon to strip off the signature part.~~ First, manually extract the ground signatures from the top 100 emails of 'kaminski-v/conferences/', and save in one file (_EnronSignatures.txt_) separated by an empty line. Also the emails from Jeb Bush folder within file '12+December+2003+Public+2.txt', and save into one file '_JebBushSignatures.txt_'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the signature corpus to train the word vector to recognize the email signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sincerely, thaleia zariphopoulou chair of the scientific committee v.n.neuhaus professor dpts of mathematics and msis the university of texas at austin', 'thank you, clare fitzgerald director, training courses marcus evans 312-540-3000x6785', 'joanna vidal events coordinator risk waters group t: (212) 925 1864 ext. 197 f: (212) 925 7585 jvidal@riskwaters.com www.riskwaters.com', 'duane seppi carnegie mellon university graduate school of industrial administrations pittsburgh, pa 15213-3890 t: 001 412-268-2298 f: 001 412-269-8896', 'helyette geman universite de paris dauphine finance department au de ka grand ecole corgy pontois, paris france 95021 t: 00 33 60-807-4200', 'vincent kaminski enron credit 1400 smith street room eb1962 houston, tx 77002-7361 t: 001 713-853-3848 f: 001 713-646-2503', 'peter nance teknecon, inc. 1515 s. capital of texas highway suite 101 austin, tx 78746 t: 001 512-732-7084 f: 001 512-732-7099', 'chris harris innogy holdings place windmill hill business park whitehill way swindon, wiltshire uk, 5n5 6pb t: 44 793 387-7777 f: 44 793 389-7811', 'spyros maragos dynergy, inc. 1000 louisiana street suite 5800 houston, tx 77002 t: 011 713-507-6589 f: 001 713-767-5958', 'ehud ronn university of texas at austin department of finance mccombs school of business austin, tx 78712-1179 t: 001 512-471-5853 f: 001 512-471-5073']\n"
     ]
    }
   ],
   "source": [
    "# read in the signature text file\n",
    "fname = \"EnronSignatures.txt\"\n",
    "fsig = open(fname, 'r')\n",
    "sigList = []\n",
    "sigText = []\n",
    "for line in fsig.readlines():\n",
    "    #print len(line)\n",
    "    if len(line) <= 3:\n",
    "        sigList.append(' '.join(sigText)) # combine all the texts of each signature\n",
    "        sigText = []\n",
    "    else:\n",
    "        sigText.append(' '.join(line.lower().split()))\n",
    "fsig.close()\n",
    "\n",
    "print(sigList[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Use word2vec to train directly\n",
    "from gensim import corpora, models, similarities\n",
    "sigTexts = []\n",
    "for term in sigList:\n",
    "    sigTexts.append([word for word in term.split()])\n",
    "model = models.word2vec.Word2Vec(sigTexts, min_count=1)\n",
    "print(model.doesnt_match('thank you but'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, extract the body(excluding email signatures) text from Enron dataset 'kaminski-v/conferences/', and use as negative training set. Try to make use of both talon and Python email packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import os\n",
    "import re\n",
    "import talon\n",
    "from talon import quotations\n",
    "from talon import signature\n",
    "from talon.signature.bruteforce import extract_signature\n",
    "import email\n",
    "from email.parser import Parser\n",
    "talon.init()\n",
    "dirName = \"conferences\"\n",
    "#filelist = [fn for fn in os.listdir(dirName) if fn != \".\"]\n",
    "#print filelist\n",
    "os.chdir(dirName)\n",
    "fcontent = open(\"content.txt\",'w')\n",
    "for fe in range(1,101):\n",
    "    fname = str(fe)+'.'\n",
    "    if not os.path.exists(fname):\n",
    "        continue\n",
    "    fmail = open(fname, 'r')\n",
    "    mailText = ''.join(fmail.readlines())\n",
    "    content = email.message_from_string(mailText)\n",
    "    if content is None:\n",
    "        continue\n",
    "    body = []\n",
    "    if content.is_multipart():\n",
    "        for payload in content.get_payload():\n",
    "            body.append(payload.get_payload())\n",
    "    else:\n",
    "        body.append(content.get_payload())\n",
    "    if body is None: # discard mail without body\n",
    "        fmail.close()\n",
    "        continue\n",
    "    #body = ''.join(body)\n",
    "    bodyText = []\n",
    "    sigSymbols = ['-- forwarded by', 'sincerely', 'kind regards', 'thank you', 'thanks']\n",
    "    for entry in body:\n",
    "        findEnd = False\n",
    "        for sym in sigSymbols:\n",
    "            if sym in entry.lower():\n",
    "                findEnd = True\n",
    "        if findEnd:\n",
    "            break\n",
    "        bodyText.append(entry)\n",
    "    reply = quotations.extract_from_plain(''.join(bodyText))\n",
    "    texts, signature = extract_signature(reply)\n",
    "    fcontent.write(texts)\n",
    "    fcontent.write('\\n')\n",
    "    fmail.close()\n",
    "fcontent.close()\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now train the model using signature text as the positive dataset and body content text as the negative dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sincerely,', 'thaleia', 'zariphopoulou', 'chair', 'of', 'the', 'scientific', 'committee', 'v.n.neuhaus', 'professor', 'dpts', 'of', 'mathematics', 'and', 'msis', 'the', 'university', 'of', 'texas', 'at', 'austin'], ['thank', 'you,', 'clare', 'fitzgerald', 'director,', 'training', 'courses', 'marcus', 'evans', '312-540-3000x6785'], ['joanna', 'vidal', 'events', 'coordinator', 'risk', 'waters', 'group', 't:', '(212)', '925', '1864', 'ext.', '197', 'f:', '(212)', '925', '7585', 'jvidal@riskwaters.com', 'www.riskwaters.com'], ['duane', 'seppi', 'carnegie', 'mellon', 'university', 'graduate', 'school', 'of', 'industrial', 'administrations', 'pittsburgh,', 'pa', '15213-3890', 't:', '001', '412-268-2298', 'f:', '001', '412-269-8896'], ['helyette', 'geman', 'universite', 'de', 'paris', 'dauphine', 'finance', 'department', 'au', 'de', 'ka', 'grand', 'ecole', 'corgy', 'pontois,', 'paris', 'france', '95021', 't:', '00', '33', '60-807-4200'], ['vincent', 'kaminski', 'enron', 'credit', '1400', 'smith', 'street', 'room', 'eb1962', 'houston,', 'tx', '77002-7361', 't:', '001', '713-853-3848', 'f:', '001', '713-646-2503'], ['peter', 'nance', 'teknecon,', 'inc.', '1515', 's.', 'capital', 'of', 'texas', 'highway', 'suite', '101', 'austin,', 'tx', '78746', 't:', '001', '512-732-7084', 'f:', '001', '512-732-7099'], ['chris', 'harris', 'innogy', 'holdings', 'place', 'windmill', 'hill', 'business', 'park', 'whitehill', 'way', 'swindon,', 'wiltshire', 'uk,', '5n5', '6pb', 't:', '44', '793', '387-7777', 'f:', '44', '793', '389-7811'], ['spyros', 'maragos', 'dynergy,', 'inc.', '1000', 'louisiana', 'street', 'suite', '5800', 'houston,', 'tx', '77002', 't:', '011', '713-507-6589', 'f:', '001', '713-767-5958'], ['ehud', 'ronn', 'university', 'of', 'texas', 'at', 'austin', 'department', 'of', 'finance', 'mccombs', 'school', 'of', 'business', 'austin,', 'tx', '78712-1179', 't:', '001', '512-471-5853', 'f:', '001', '512-471-5073']]\n",
      "[['please', 'register', 'wincenty', 'j.', '(vince)', 'kaminski,', 'managing', 'director,', 'research'], ['enron', 'wholesale', 'services,', 'to', 'the', 'subject', 'conference', 'to', 'be', 'held', 'in', 'houston'], ['on', 'june', '22,', '2001.'], ['if', 'you', 'need', 'more', 'information,', 'please', 'contact', 'me', 'at:'], ['dear', 'mr.', 'ray,'], ['i', 'regret', 'to', 'inform', 'you', 'that', 'due', 'to', 'very', 'heavy', 'workload', 'we', 'cannot', 'attend'], ['the', 'power', 'systems', 'engineering', 'research', \"center's\"], ['upcoming', 'industrial', 'advisory', 'board', 'meeting', 'in', 'oak', 'brook.'], ['our', 'work', 'load', 'does', 'not', 'leave', 'us', 'much', 'time', 'to', 'get', 'involved'], ['with', 'pserc', 'at', 'this', 'moment.', 'we', 'would', 'very', 'much', 'like', 'to', 'stay']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec\n",
    "import collections\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.linear_model\n",
    "import nltk\n",
    "def load_data(path_to_data):\n",
    "    train_pos = []\n",
    "    train_neg = []\n",
    "    sigwords = []\n",
    "    with open(path_to_data+\"EnronSignatures.txt\",'r') as fes:\n",
    "        for line in fes:\n",
    "            if len(line) <= 3:\n",
    "                train_pos.append((' '.join(sigwords)).split()) # combine all the texts of each signature\n",
    "                sigwords = []\n",
    "            else:\n",
    "                sigwords.append(' '.join(line.lower().strip().split()))\n",
    "    with open(path_to_data+\"train_content.txt\",'r') as ftc:\n",
    "        for line in ftc:\n",
    "            words = [w.lower() for w in line.strip().split()]\n",
    "            if len(words) < 2:\n",
    "                continue\n",
    "            train_neg.append(words)\n",
    "    return train_pos, train_neg\n",
    "\n",
    "train_pos, train_neg = load_data('./')\n",
    "print train_pos[:10]\n",
    "print train_neg[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features for training process using gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 0\n",
      "Training iteration 1\n",
      "Training iteration 2\n",
      "Training iteration 3\n",
      "Training iteration 4\n"
     ]
    }
   ],
   "source": [
    "def feature_extraction(train_pos, train_neg):\n",
    "    labeled_train_pos = []\n",
    "    for index, words in enumerate(train_pos):\n",
    "        sentence = LabeledSentence(words, [\"TRAIN_POS_%s\"%index])\n",
    "        labeled_train_pos.append(sentence)\n",
    "    labeled_train_neg = []\n",
    "    for index, words in enumerate(train_neg):\n",
    "        sentence = LabeledSentence(words, [\"TRAIN_NEG_%s\"%index])\n",
    "        labeled_train_neg.append(sentence)\n",
    "    model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=4)\n",
    "    sentences = labeled_train_pos + labeled_train_neg\n",
    "    model.build_vocab(sentences)\n",
    "    for i in range(5):\n",
    "        print \"Training iteration %d\" %(i)\n",
    "        random.shuffle(sentences)\n",
    "        model.train(sentences)\n",
    "    train_pos_vec, train_neg_vec = [], []\n",
    "    for index in range(len(labeled_train_pos)):\n",
    "        doc_vec = model.docvecs[\"TRAIN_POS_%s\"%index]\n",
    "        train_pos_vec.append(doc_vec)\n",
    "    for index in range(len(labeled_train_neg)):\n",
    "        doc_vec = model.docvecs[\"TRAIN_NEG_%s\"%index]\n",
    "        train_neg_vec.append(doc_vec)\n",
    "    return train_pos_vec, train_neg_vec\n",
    "\n",
    "train_pos_vec, train_neg_vec = feature_extraction(train_pos, train_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the transformed vectors to build model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(train_pos_vec, train_neg_vec):\n",
    "    Y = [\"pos\"]*len(train_pos_vec) + [\"neg\"]*len(train_neg_vec)\n",
    "    X = train_pos_vec + train_neg_vec\n",
    "    lr_model = sklearn.linear_model.LogisticRegression()\n",
    "    lr_model.fit(X,Y)\n",
    "    return lr_model\n",
    "\n",
    "lr_model = build_model(train_pos_vec, train_neg_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model using the training (testing) data by presenting the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:\tpos\tneg\n",
      "actual:\n",
      "pos\t\t0\t45\n",
      "neg\t\t0\t338\n",
      "accuracy: 0.500000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_pos_vec, test_neg_vec, print_confusion=False):\n",
    "    test_pos_predict = model.predict(test_pos_vec)\n",
    "    test_neg_predict = model.predict(test_neg_vec)\n",
    "    test_pos_Y = [\"pos\"]*len(test_pos_vec)\n",
    "    test_neg_Y = [\"neg\"]*len(test_neg_vec)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for i in range(len(test_pos_predict)):\n",
    "        if test_pos_predict[i] == test_pos_Y[i]:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    for i in range(len(test_neg_predict)):\n",
    "        if test_neg_predict[i] == test_neg_Y[i]:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    accuracy = float(tp+tn) / float(tp+tn+tp+tn)\n",
    "    precision = float(tp) / float(tp+fp)\n",
    "    recall = float(tp) / float(tp+fn)\n",
    "    if print_confusion:\n",
    "        print \"predicted:\\tpos\\tneg\"\n",
    "        print \"actual:\"\n",
    "        print \"pos\\t\\t%d\\t%d\" % (tp, fn)\n",
    "        print \"neg\\t\\t%d\\t%d\" % (fp, tn)\n",
    "    print \"accuracy: %f\" % (accuracy)\n",
    "    print \"precision: %f\" % (precision)\n",
    "    print \"recall: %f\" % (recall)\n",
    "    \n",
    "evaluate_model(lr_model, train_pos_vec, train_neg_vec, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
