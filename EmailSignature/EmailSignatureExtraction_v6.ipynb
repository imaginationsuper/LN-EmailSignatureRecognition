{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix mistakes in the feature_extraction to approach real case, but still will change vocabulary building procedure once we obtain larger dataset.\n",
    "## Also will apply new minority adjustment method using SMOTE.\n",
    "\n",
    "\n",
    "In this version, it fixes some training strategy and apply multiple classifiers for the future ensemble classifier. \n",
    "Here are some prior assumptions I made:\n",
    "- Email signatures and bodies class are balanced in the overall dataset. (not in a real case)\n",
    "- Training set are majority and the split ratio is between 0.6 and 0.9. \n",
    "- In my case, I convert the whole dataset at once due to the limited quantity of dataset size, while in real case it should use the bag-of-words from training set to express test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging # record log event\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load positive(email signatures) and negative(email body contents) data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sincerely'], ['thaleia', 'zariphopoulou'], ['chair', 'of', 'the', 'scientific', 'committee'], ['v', 'n', 'neuhaus', 'professor'], ['dpts', 'of', 'mathematics', 'and', 'msis']]\n",
      "[['please', 'register', 'wincenty', 'j', 'vince', 'kaminski', 'managing', 'director', 'research'], ['enron', 'wholesale', 'services', 'to', 'the', 'subject', 'conference', 'to', 'be', 'held', 'in', 'houston'], ['on', 'june'], ['if', 'you', 'need', 'more', 'information', 'please', 'contact', 'me', 'at'], ['dear', 'mr', 'ray']]\n",
      "338 338 338\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def load_data(path_to_data):\n",
    "    train_pos = []\n",
    "    train_neg = []\n",
    "    sigwords = []\n",
    "    with open(path_to_data+\"TotalSignatures.txt\",'r') as fes:\n",
    "        for line in fes:\n",
    "            sigwords = line.lower().split()#\n",
    "            train_pos.append((' '.join(sigwords)).split()) #\n",
    "    with open(path_to_data+\"train_content.txt\",'r') as ftc:\n",
    "        for line in ftc:\n",
    "            words = [w.lower() for w in line.strip().split()]\n",
    "            if len(words) < 2:\n",
    "                continue\n",
    "            train_neg.append(words)\n",
    "    return train_pos, train_neg\n",
    "\n",
    "def data_filter(raw_text_data):\n",
    "    clean_text_data = []\n",
    "    for entry in raw_text_data:\n",
    "        clean_entry = re.sub(r\"\\n|(\\\\(.*?){)|}|[!$%^&*#()_+|~\\-={}\\[\\]:\\\";'<>?,.\\/\\\\]|[0-9]|[@]\", ' ', ' '.join(entry))\n",
    "        #clean_entry = re.sub(r\"\\n|(\\\\(.*?){)|}|[!$%^&*#_+|\\={}\\[\\]\\\";'<>?,.\\/]|[0-9]\", ' ', ' '.join(entry))\n",
    "        #clean_entry = ' '.join(entry)\n",
    "        clean_entry = re.sub('\\s+', ' ', clean_entry)\n",
    "        clean_text_data.append(clean_entry.split())\n",
    "    return clean_text_data\n",
    "\n",
    "data_pos, data_neg = load_data('./')\n",
    "data_pos = data_filter(data_pos)\n",
    "data_neg = data_filter(data_neg)\n",
    "data_size = min(len(data_pos), len(data_neg))\n",
    "data_pos = data_pos[:data_size]\n",
    "data_neg = data_neg[:data_size]\n",
    "print data_pos[:5]\n",
    "print data_neg[:5]\n",
    "print data_size, len(data_pos), len(data_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features using gensim models based on word vectors, and split the transformed dataset into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 0\n",
      "Training iteration 1\n",
      "Training iteration 2\n",
      "Training iteration 3\n",
      "Training iteration 4\n",
      "data_size = 338\n",
      "training_size = 270\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec\n",
    "def feature_extraction_Doc2Vec(data_pos, data_neg): # use the word2vec under the hood\n",
    "    labeled_data_pos = []\n",
    "    for index, words in enumerate(data_pos):\n",
    "        sentence = LabeledSentence(words, [\"DATA_POS_%s\"%index])\n",
    "        labeled_data_pos.append(sentence)\n",
    "    labeled_data_neg = []\n",
    "    for index, words in enumerate(data_neg):\n",
    "        sentence = LabeledSentence(words, [\"DATA_NEG_%s\"%index])\n",
    "        labeled_data_neg.append(sentence)\n",
    "    model = Doc2Vec(min_count=1, window=20, size=4000, sample=1e-4, negative=5, workers=4)\n",
    "    sentences = labeled_data_pos + labeled_data_neg\n",
    "    model.build_vocab(sentences)\n",
    "    for i in range(5):\n",
    "        print \"Training iteration %d\" %(i)\n",
    "        random.shuffle(sentences)\n",
    "        model.train(sentences)\n",
    "    data_pos_vec, data_neg_vec = [], []\n",
    "    for index in range(len(labeled_data_pos)):\n",
    "        doc_vec = model.docvecs[\"DATA_POS_%s\"%index]\n",
    "        data_pos_vec.append(doc_vec)\n",
    "    for index in range(len(labeled_data_neg)):\n",
    "        doc_vec = model.docvecs[\"DATA_NEG_%s\"%index]\n",
    "        data_neg_vec.append(doc_vec)\n",
    "    return data_pos_vec, data_neg_vec\n",
    "\n",
    "data_pos_vec, data_neg_vec = feature_extraction_Doc2Vec(data_pos, data_neg)\n",
    "split_ratio = 0.8\n",
    "cutoff = int(math.floor(data_size*split_ratio))\n",
    "random.shuffle(data_pos_vec)\n",
    "random.shuffle(data_neg_vec)\n",
    "train_pos_vec = data_pos_vec[:cutoff]\n",
    "train_neg_vec = data_neg_vec[:cutoff]\n",
    "test_pos_vec = data_pos_vec[cutoff:]\n",
    "test_neg_vec = data_neg_vec[cutoff:]\n",
    "#test_pos_vec, test_neg_vec = feature_extraction_Doc2Vec(test_pos, test_neg)\n",
    "print \"data_size = %d\" % data_size\n",
    "print \"training_size = %d\" % cutoff\n",
    "print(len(train_pos_vec[0]))\n",
    "#print(map(list,test_pos_vec[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply binary classifiers to find signature lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes\n",
    "import sklearn.linear_model\n",
    "import sklearn.svm\n",
    "import sklearn.neighbors.nearest_centroid\n",
    "import sklearn.tree\n",
    "def build_model(train_pos_vec, train_neg_vec):\n",
    "    Y = [\"pos\"]*len(train_pos_vec) + [\"neg\"]*len(train_neg_vec)\n",
    "    X = train_pos_vec + train_neg_vec\n",
    "    # use multiple classification methods\n",
    "    svm_model = sklearn.svm.SVC() # SVM\n",
    "    svm_model.fit(X,Y)\n",
    "    nnc_model = sklearn.neighbors.nearest_centroid.NearestCentroid() # Nearest Neighbor\n",
    "    nnc_model.fit(X,Y)\n",
    "    lr_model = sklearn.linear_model.LogisticRegression() # Logistic Regression\n",
    "    lr_model.fit(X,Y)\n",
    "    nb_model = sklearn.naive_bayes.GaussianNB() # Naive Bayes\n",
    "    nb_model.fit(X,Y)\n",
    "    dt_model = sklearn.tree.DecisionTreeClassifier()\n",
    "    dt_model.fit(X,Y)\n",
    "    return svm_model, nnc_model, lr_model, nb_model, dt_model\n",
    "\n",
    "svm_model, nnc_model, lr_model, nb_model, dt_model = build_model(train_pos_vec, train_neg_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate all the models listed by confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "predicted:\tpos\tneg\n",
      "actual:\n",
      "pos\t\t49\t19\n",
      "neg\t\t38\t30\n",
      "accuracy: 0.576642\n",
      "precision: 0.556818\n",
      "recall: 0.710145\n",
      "Fscore: 0.624204\n",
      "\n",
      "\n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "predicted:\tpos\tneg\n",
      "actual:\n",
      "pos\t\t41\t27\n",
      "neg\t\t32\t36\n",
      "accuracy: 0.562044\n",
      "precision: 0.554054\n",
      "recall: 0.594203\n",
      "Fscore: 0.573427\n",
      "\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "predicted:\tpos\tneg\n",
      "actual:\n",
      "pos\t\t40\t28\n",
      "neg\t\t32\t36\n",
      "accuracy: 0.554745\n",
      "precision: 0.547945\n",
      "recall: 0.579710\n",
      "Fscore: 0.563380\n",
      "\n",
      "\n",
      "GaussianNB()\n",
      "predicted:\tpos\tneg\n",
      "actual:\n",
      "pos\t\t40\t28\n",
      "neg\t\t33\t35\n",
      "accuracy: 0.547445\n",
      "precision: 0.540541\n",
      "recall: 0.579710\n",
      "Fscore: 0.559441\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best')\n",
      "predicted:\tpos\tneg\n",
      "actual:\n",
      "pos\t\t41\t27\n",
      "neg\t\t37\t31\n",
      "accuracy: 0.525547\n",
      "precision: 0.518987\n",
      "recall: 0.594203\n",
      "Fscore: 0.554054\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_pos_vec, test_neg_vec, print_confusion=False):\n",
    "    test_pos_predict = model.predict(test_pos_vec)\n",
    "    test_neg_predict = model.predict(test_neg_vec)\n",
    "    test_pos_Y = [\"pos\"]*len(test_pos_vec)\n",
    "    test_neg_Y = [\"neg\"]*len(test_neg_vec)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for i in range(len(test_pos_predict)):\n",
    "        if test_pos_predict[i] == test_pos_Y[i]:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    for i in range(len(test_neg_predict)):\n",
    "        if test_neg_predict[i] == test_neg_Y[i]:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    accuracy = float(tp+tn) / float(tp+tn+fp+fn+1)\n",
    "    precision = float(tp) / float(tp+fp+1)\n",
    "    recall = float(tp) / float(tp+fn+1)\n",
    "    Fscore = (2*recall*precision) / (recall + precision)\n",
    "    print str(model)\n",
    "    if print_confusion:\n",
    "        print \"predicted:\\tpos\\tneg\"\n",
    "        print \"actual:\"\n",
    "        print \"pos\\t\\t%d\\t%d\" % (tp, fn)\n",
    "        print \"neg\\t\\t%d\\t%d\" % (fp, tn)\n",
    "    print \"accuracy: %f\" % (accuracy)\n",
    "    print \"precision: %f\" % (precision)\n",
    "    print \"recall: %f\" % (recall)\n",
    "    print \"Fscore: %f\" % (Fscore)\n",
    "    print '\\n'\n",
    "\n",
    "evaluate_model(svm_model, test_pos_vec, test_neg_vec, True)\n",
    "evaluate_model(nnc_model, test_pos_vec, test_neg_vec, True)\n",
    "evaluate_model(lr_model, test_pos_vec, test_neg_vec, True)\n",
    "evaluate_model(nb_model, test_pos_vec, test_neg_vec, True)\n",
    "evaluate_model(dt_model, test_pos_vec, test_neg_vec, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference site:\n",
    "- [gensim Doc2Vec API](https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument)\n",
    "- [Doc2Vec Tutorial](http://rare-technologies.com/doc2vec-tutorial/)\n",
    "- [Scikit-learn Classifier](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
